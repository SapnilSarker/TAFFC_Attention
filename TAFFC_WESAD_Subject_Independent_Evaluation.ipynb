{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6055182,"sourceType":"datasetVersion","datasetId":3445338}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up the GPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check for GPU and set memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPUs are available and memory growth is set\")\n    except RuntimeError as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T05:50:35.540906Z","iopub.execute_input":"2024-11-17T05:50:35.541204Z","iopub.status.idle":"2024-11-17T05:50:46.628228Z","shell.execute_reply.started":"2024-11-17T05:50:35.541177Z","shell.execute_reply":"2024-11-17T05:50:46.627295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the ECG data for all subjects","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# List to store individual subject DataFrames\ndata_frames = []\n\n# Iterate over the subject numbers\nfor subject_number in range(2, 18):\n    # Construct the file path for each subject\n    file_path = f'/kaggle/input/wesad-dataset/S{subject_number}_respiban.txt'\n    \n    # Check if the file exists\n    if os.path.exists(file_path):\n        # Read the file into a DataFrame\n        df = pd.read_csv(file_path, delimiter='\\t', skiprows=3, header=None)\n        # Select the first three columns\n        df_subset = df.iloc[:, 2]\n        # Append the DataFrame to the list\n        print(df_subset.shape)\n \n        df_subset.columns = [f'Subject_{subject_number}']\n        \n        data_frames.append(df_subset)\n    else:\n        print(f'File not found for subject {subject_number}')\n\n# Concatenate all DataFrames into a single DataFrame\ndata_1 = pd.concat(data_frames,axis=1)\n\n# Print the shape of the combined data\nprint(data_1.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-18T12:33:16.698065Z","iopub.execute_input":"2024-11-18T12:33:16.698341Z","iopub.status.idle":"2024-11-18T12:35:03.050557Z","shell.execute_reply.started":"2024-11-18T12:33:16.698316Z","shell.execute_reply":"2024-11-18T12:35:03.049514Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The ECG signal required to have some modification to converting it from raw signal values to SI unit.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n# Define the constants\nchan_bit = 2 ** 16\nvcc = 3\n# Apply the equation to the dataset\ndata_wesad = data_1.applymap(lambda x: ((x / chan_bit - 0.5) * vcc) if not np.isnan(x) else np.nan)\n# Print the updated dataset\nprint(data_wesad)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:35:33.413311Z","iopub.execute_input":"2024-11-18T12:35:33.413997Z","iopub.status.idle":"2024-11-18T12:38:01.957853Z","shell.execute_reply.started":"2024-11-18T12:35:33.413957Z","shell.execute_reply":"2024-11-18T12:38:01.956915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sample plotting of the ECG signal","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\nsubject_data = data_wesad.iloc[:550, 4]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(550)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:40:10.078190Z","iopub.execute_input":"2024-11-18T12:40:10.078944Z","iopub.status.idle":"2024-11-18T12:40:10.355983Z","shell.execute_reply.started":"2024-11-18T12:40:10.078912Z","shell.execute_reply":"2024-11-18T12:40:10.355067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the start and end of every session(stress/baseline) for each subjects. The file WESAD_mins.xlsx was made by the author(based on the WESAD documentation) for the simplification of the process.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your .xlsx file\nfile_path = '/kaggle/input/wesad-dataset/WESAD_mins.xlsx'\n\n# Read the .xlsx file into a DataFrame\ndf = pd.read_excel(file_path)\ndf_sub = df.iloc[:, 1:]\n# Print the DataFrame\nprint(df_sub)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:40:17.252313Z","iopub.execute_input":"2024-11-18T12:40:17.252693Z","iopub.status.idle":"2024-11-18T12:40:17.285409Z","shell.execute_reply.started":"2024-11-18T12:40:17.252662Z","shell.execute_reply":"2024-11-18T12:40:17.284569Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting up the data points according to the baseline and stress time","metadata":{}},{"cell_type":"code","source":"num_intervals_base = 119\nnum_intervals_tsst = 59\n\nbase_interval = (df_sub['Base_end'] - df_sub['Base_start']) / (num_intervals_base + 1)\ntsst_interval = (df_sub['Tsst_end'] - df_sub['TSST_Start']) / (num_intervals_tsst + 1)\n\n# Create the new dataset\nnew_df = pd.DataFrame()\nnew_df['base_start'] = df_sub['Base_start']\nfor i in range(1, num_intervals_base + 1):\n    new_df[f'base_s{i}'] = df_sub['Base_start'] + i * base_interval\nnew_df['base_end'] = df_sub['Base_end']\nnew_df['tsst_start'] = df_sub['TSST_Start']\nfor i in range(1, num_intervals_tsst + 1):\n    new_df[f'tsst_s{i}'] = df_sub['TSST_Start'] + i * tsst_interval\nnew_df['tsst_end'] = df_sub['Tsst_end']\ndf_lebel=(new_df*700*60).T\n# Print the new dataset\nprint(df_lebel.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:40:41.383153Z","iopub.execute_input":"2024-11-18T12:40:41.383513Z","iopub.status.idle":"2024-11-18T12:40:41.505235Z","shell.execute_reply.started":"2024-11-18T12:40:41.383487Z","shell.execute_reply":"2024-11-18T12:40:41.504412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observing the absolute lowest session segment data points in the dataset.","metadata":{}},{"cell_type":"code","source":"min_diffs = df_lebel.diff(axis=0).abs().min()\n\n# Find the absolute lowest value among the minimum differences\nabsolute_lowest = np.floor(min_diffs.min()).astype(int)\n\n# Print the absolute lowest value\nprint(absolute_lowest)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:40:50.503093Z","iopub.execute_input":"2024-11-18T12:40:50.503377Z","iopub.status.idle":"2024-11-18T12:40:50.509780Z","shell.execute_reply.started":"2024-11-18T12:40:50.503341Z","shell.execute_reply":"2024-11-18T12:40:50.508849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Slicing every subjects data of same shape to avoid the mismatch in the data point. Also setting up the label.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nsamples = []\nlebels = []\n# Iterate over the subjects\nfor subject_number in range(15):\n    # Read the first dataset for the current subject\n    df1 = data_wesad.iloc[:, subject_number] # Replace with your own file path\n    df2 = df_lebel.iloc[:,subject_number]\n    #print(subject_number)\n    indices = df2.values.astype(int).flatten()\n    # Iterate over the indices\n    for i in range(len(indices) - 1):\n        if i != 20:  # Exclude the 5th sample\n            start = indices[i]\n            end = start+absolute_lowest\n            sample = df1.iloc[start-1:end].values  # Cut the sample from the first dataset\n            sample = pd.Series(sample)\n            samples.append(sample)\n            #print(sample.shape)\n    lebel=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n    lebels = lebels+lebel\n\n# Concatenate the samples along a new axis\nconcatenated = pd.concat(samples,axis=1).T\nlebel_all = pd.DataFrame(lebels)\n\n# Print the shape of the concatenated dataset\nprint(concatenated.shape)\nprint(lebel_all.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:09.735335Z","iopub.execute_input":"2024-11-18T13:46:09.735698Z","iopub.status.idle":"2024-11-18T13:46:10.770591Z","shell.execute_reply.started":"2024-11-18T13:46:09.735669Z","shell.execute_reply":"2024-11-18T13:46:10.769626Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downsamplin the dataset from (700 Hz to 256 Hz to reduce the computationl cost and the improvement of the performance","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import resample\noriginal_dataset = concatenated.T\n\n# Assuming original_dataset has shape (450, 40608)\ndownsampled_dataset = np.zeros((2700, 256 * (6768 // 700)))  # Downsampling to 256 Hz\nprint(original_dataset[0].shape)\nfor i in range(original_dataset.shape[1]):\n    original_signal = original_dataset[i]\n    downsampled_signal = resample(original_signal, 256 * (6768 // 700))\n    downsampled_dataset[i, :] = downsampled_signal\nwesad_x=downsampled_dataset.T\nprint(wesad_x.shape)  # Output: (90, 58418)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:13.959194Z","iopub.execute_input":"2024-11-18T13:46:13.959579Z","iopub.status.idle":"2024-11-18T13:46:14.683224Z","shell.execute_reply.started":"2024-11-18T13:46:13.959547Z","shell.execute_reply":"2024-11-18T13:46:14.682306Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observing the downsampled dataset.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\nwesad_x = pd.DataFrame(wesad_x)\nsubject_data = wesad_x.iloc[:200, 4]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(200)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:17.525740Z","iopub.execute_input":"2024-11-18T13:46:17.526391Z","iopub.status.idle":"2024-11-18T13:46:17.795683Z","shell.execute_reply.started":"2024-11-18T13:46:17.526339Z","shell.execute_reply":"2024-11-18T13:46:17.794764Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transposing the dataset.","metadata":{}},{"cell_type":"code","source":"X= wesad_x.T\ny=lebel_all","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:23.038686Z","iopub.execute_input":"2024-11-18T13:46:23.039027Z","iopub.status.idle":"2024-11-18T13:46:23.044081Z","shell.execute_reply.started":"2024-11-18T13:46:23.039000Z","shell.execute_reply":"2024-11-18T13:46:23.043057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D, Attention, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\ninput_layer = Input(shape=(2304, 1))\n\n# Shared CNN layers\ndef shared_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    conv1 = Conv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n\n    conv2 = Conv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n\n    conv3 = Conv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n\n    return batch_norm3\n\n# Shared BiLSTM layer with attention\ndef shared_bilstm_with_attention(shared_cnn_output, units, return_sequences, num_heads):\n    bilstm_output = Bidirectional(LSTM(units=units, return_sequences=return_sequences))(shared_cnn_output)\n    attention_heads = []\n    for _ in range(num_heads):\n        attention_head = Attention()([bilstm_output, bilstm_output])\n        attention_heads.append(attention_head)\n\n    # Concatenate the outputs of attention heads\n    multi_attention = Concatenate(axis=-1)(attention_heads)\n    global_pool = GlobalMaxPooling1D()(multi_attention)\n    return global_pool\n\n# Apply shared CNN and BiLSTM with attention to each input path with different parameters\nshared_cnn_output1 = shared_cnn(input_layer, filters1=64, kernel_size1=10, filters2=128, kernel_size2=5, filters3=256, kernel_size3=3)\nshared_bilstm_output1 = shared_bilstm_with_attention(shared_cnn_output1, units=64, return_sequences=True, num_heads=4)\n\nshared_cnn_output2 = shared_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\nshared_bilstm_output2 = shared_bilstm_with_attention(shared_cnn_output2, units=32, return_sequences=True, num_heads=2)\n\n# Concatenate the outputs of both paths\nconcatenated_output = Concatenate(axis=-1)([shared_bilstm_output1, shared_bilstm_output2])\n\n# Dense layers\ndense1 = Dense(units=256, activation='relu', kernel_regularizer=l2(0.01))(concatenated_output)\nbatch_norm4 = BatchNormalization()(dense1)\ndrop1 = Dropout(0.4)(batch_norm4)\n\ndense2 = Dense(units=128, activation='relu', kernel_regularizer=l2(0.01))(drop1)\nbatch_norm5 = BatchNormalization()(dense2)\ndrop2 = Dropout(0.4)(batch_norm5)\n\n# Output layer\noutput = Dense(units=1, activation='sigmoid')(drop2)\n\n# Model\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:26.281775Z","iopub.execute_input":"2024-11-18T13:46:26.282347Z","iopub.status.idle":"2024-11-18T13:46:27.619948Z","shell.execute_reply.started":"2024-11-18T13:46:26.282312Z","shell.execute_reply":"2024-11-18T13:46:27.619029Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# from here the LOOCV special code starts. ","metadata":{}},{"cell_type":"code","source":"features= wesad_x.T\nprint(features.shape)\nlabels=lebel_all\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:46:51.383255Z","iopub.execute_input":"2024-11-18T13:46:51.384099Z","iopub.status.idle":"2024-11-18T13:46:51.389768Z","shell.execute_reply.started":"2024-11-18T13:46:51.384063Z","shell.execute_reply":"2024-11-18T13:46:51.388874Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Here, LOOCV test-train splitting (only 1 subject is considered as validation data. the rests are as train data.) is done. Subjects =[1 to 17] sub = 1 means subject 1 of the Vollmer dataset will be considered as test data, while other subjects will be considered as train data.","metadata":{}},{"cell_type":"code","source":"\ndef generate_train_test_data(test_number, test_interval=180):\n    if test_number <= 0:\n        raise ValueError(\"Test number should be greater than 0.\")\n    #if test_number * test_interval > len(features):\n        #raise ValueError(\"Test number exceeds the total number of datasets.\")\n\n    start_idx = (test_number - 1) * test_interval\n    end_idx = start_idx + test_interval\n\n \n    test_indices = list(range(start_idx, end_idx))\n    print(test_indices)\n    train_indices = list(range(len(features)))\n    train_indices = [i for i in train_indices if i not in test_indices]\n\n    #print(test_slice)\n    #print(train_indices)\n\n    X_test = features.iloc[test_indices,:]\n    #print(X_test.shape)\n    y_test = labels.iloc[test_indices]\n    \n    X_train = features.iloc[train_indices,:]\n    y_train = labels.iloc[train_indices,:]\n\n    return X_train, X_test, y_train, y_test\n\n# Example usage for getting train and test data for the first 6 datasets  5  9\nX_train, X_test, y_train, y_test = generate_train_test_data(test_number=13)\n\n# Now, perform the standard scaling as before\nfrom sklearn.preprocessing import StandardScaler\n\nscaling = StandardScaler()\nX_train = scaling.fit_transform(X_train)\nX_test = scaling.transform(X_test)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:32:29.340017Z","iopub.execute_input":"2024-11-18T14:32:29.340394Z","iopub.status.idle":"2024-11-18T14:32:29.889915Z","shell.execute_reply.started":"2024-11-18T14:32:29.340345Z","shell.execute_reply":"2024-11-18T14:32:29.888990Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_flatten = y_train.values.flatten()\n\nX_majority = X_train[y_train_flatten == 0, :]\nX_minority = X_train[y_train_flatten == 1, :]\nprint(X_minority.shape)\nprint(X_majority.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:32:40.456183Z","iopub.execute_input":"2024-11-18T14:32:40.456893Z","iopub.status.idle":"2024-11-18T14:32:40.474837Z","shell.execute_reply.started":"2024-11-18T14:32:40.456858Z","shell.execute_reply":"2024-11-18T14:32:40.473856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train data augmentation","metadata":{}},{"cell_type":"code","source":"X_minority_augmented = np.zeros((2*X_minority.shape[0], 2304))\nprint(X_minority_augmented.shape)\nj=0\ni=0\n#print(int(X_minority.shape[0]/2))\nfor i in range(int(X_minority.shape[0]/2)):\n    i = i*2\n    #print(i)\n    x1=X_minority[i,:]\n    x2=X_minority[i+1,:]\n    x_combined = np.concatenate((X_minority[i,:], X_minority[i+1,:]))\n    #print(x_combined.shape)\n    x3 = x_combined[768:3072]\n    x4 = x_combined[1792:4096]\n    #print(x3.shape)\n    #print(x2.shape)\n        \n    X_minority_augmented[j] = x1  #np.roll(x1, 2000)\n    j=j+1    \n    X_minority_augmented[j] = x2  #np.roll(x1, 2000)\n    j=j+1\n    X_minority_augmented[j] = x3  #np.roll(x1, 2000)\n    j=j+1\n    #x1=X_minority[i,:]\n    X_minority_augmented[j] = x4 #np.roll(x1, 2000)x1\n    j=j+1\n    #i=i+1\n    #print(i)\n#print(i)\nprint(X_minority_augmented.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:32:44.153450Z","iopub.execute_input":"2024-11-18T14:32:44.153822Z","iopub.status.idle":"2024-11-18T14:32:44.190248Z","shell.execute_reply.started":"2024-11-18T14:32:44.153790Z","shell.execute_reply":"2024-11-18T14:32:44.189297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_augmented = np.vstack((X_majority, X_minority_augmented))\ny_train_augmented = np.hstack((np.zeros(X_majority.shape[0]), np.ones(X_minority_augmented.shape[0])))\n# shuffle the data\n\nidx = np.random.permutation(X_train_augmented.shape[0])\n#print(idx)\nX_train_augmented = X_train_augmented[idx]\ny_train_augmented = pd.DataFrame(y_train_augmented[idx])\nprint(X_train_augmented.shape)\n#print(y_train_augmented[0])\nX_train=X_train_augmented\ny_train=y_train_augmented","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:32:49.110859Z","iopub.execute_input":"2024-11-18T14:32:49.111727Z","iopub.status.idle":"2024-11-18T14:32:49.154350Z","shell.execute_reply.started":"2024-11-18T14:32:49.111692Z","shell.execute_reply":"2024-11-18T14:32:49.153417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training of the model","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\n# Define early stopping and checkpoint callbacks\nes = EarlyStopping(monitor='val_loss', mode='min', patience=100)\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n# Train the model with early stopping and checkpoint callbacks\nhistory = model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_test, y_test), callbacks=[es, mc])","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:34:06.865046Z","iopub.execute_input":"2024-11-18T14:34:06.865830Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# calculating the confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom keras.models import load_model\n# Load the saved model\nmodel = load_model('best_model.h5')\n\n# Predict the class probabilities for the test set\ny_pred = model.predict(X_test)\n\n# Convert the probabilities into class labels using a threshold of 0.5\ny_pred_classes = (y_pred > 0.5).astype(int)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred_classes)\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:10:33.222957Z","iopub.execute_input":"2024-11-18T14:10:33.223840Z","iopub.status.idle":"2024-11-18T14:10:36.591274Z","shell.execute_reply.started":"2024-11-18T14:10:33.223806Z","shell.execute_reply":"2024-11-18T14:10:36.590377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# loss vs epoch graph","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# Print loss vs. epoch curve\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:10:46.683943Z","iopub.execute_input":"2024-11-18T14:10:46.684653Z","iopub.status.idle":"2024-11-18T14:10:46.966570Z","shell.execute_reply.started":"2024-11-18T14:10:46.684611Z","shell.execute_reply":"2024-11-18T14:10:46.965667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# auc score calculation","metadata":{}},{"cell_type":"code","source":"# Calculate predictions for the test set\ny_pred = model.predict(X_test)\n\n# Calculate AUC score\nauc_score = roc_auc_score(y_test, y_pred)\n\n# Print AUC score\nprint('AUC Score:', auc_score)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:10:51.878078Z","iopub.execute_input":"2024-11-18T14:10:51.878452Z","iopub.status.idle":"2024-11-18T14:10:52.133154Z","shell.execute_reply.started":"2024-11-18T14:10:51.878420Z","shell.execute_reply":"2024-11-18T14:10:52.132281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ROC curve plotting","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Calculate predictions for the test set\ny_pred = model.predict(X_test)\n\n# Calculate false positive rate, true positive rate, and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n# Calculate AUC score\nauc_score = roc_auc_score(y_test, y_pred)\n\n# Plot ROC curve\nplt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc_score))\nplt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:30:23.558169Z","iopub.execute_input":"2024-11-18T14:30:23.558483Z","iopub.status.idle":"2024-11-18T14:30:24.080359Z","shell.execute_reply.started":"2024-11-18T14:30:23.558455Z","shell.execute_reply":"2024-11-18T14:30:24.079493Z"},"trusted":true},"outputs":[],"execution_count":null}]}