{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6688097,"sourceType":"datasetVersion","datasetId":3857265}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\n# Check for GPU and set memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPUs are available and memory growth is set\")\n    except RuntimeError as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T18:53:57.474267Z","iopub.execute_input":"2024-11-11T18:53:57.475187Z","iopub.status.idle":"2024-11-11T18:53:57.481447Z","shell.execute_reply.started":"2024-11-11T18:53:57.475151Z","shell.execute_reply":"2024-11-11T18:53:57.480318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Specify the directory where the files are located\ndirectory = \"/kaggle/input/vollmer-csv/vollmer_s0\"  # Replace with the actual directory path\n\n# Create an empty list to store the data from the CSV files\ndf = pd.DataFrame()\ndata = []\n# Loop through the desired file numbers (1 to 13) with two digits\nfor file_number in range(1, 14):\n    # Use string formatting to ensure two-digit file numbers\n    file_name = f\"{file_number:02d}.csv\"\n    #print(file_name)\n    file_path = directory + file_name\n    #print(file_path)\n    data = pd.read_csv(file_path)\n    third_column = data.iloc[:, 4] \n    #print(third_column.shape)\n    df = pd.concat([df, third_column], axis=1, ignore_index=True)\n    #print(df.shape)\n\n# Now, data_list contains the data from the 13 specific CSV files with two-digit file numbers.\n\nprint(df.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:33:52.869267Z","iopub.execute_input":"2024-11-19T08:33:52.869527Z","iopub.status.idle":"2024-11-19T08:34:03.917478Z","shell.execute_reply.started":"2024-11-19T08:33:52.869507Z","shell.execute_reply":"2024-11-19T08:34:03.916649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Specify the directory where the files are located\ndirectory = \"/kaggle/input/vollmer-csv/vollmer_labels_s0\"  # Replace with the actual directory path\n\n# Create an empty list to store the data from the CSV files\ncut_points = pd.DataFrame()\n\n# Loop through the desired file numbers (1 to 13) with two digits\nfor file_number in range(1, 14):\n    # Use string formatting to ensure two-digit file numbers\n    file_name = f\"{file_number:02d}.csv\"\n    #print(file_name)\n    file_path = directory + file_name\n    #print(file_path)\n    cut = pd.read_csv(file_path)\n    #print(cut)\n    selected_columns = cut[['FAROS_Marker/Rest', 'FAROS_Marker/Walking', 'FAROS_Marker/2-Back','Manual/Running']]\n\n    #print(selected_columns3)\n    cut_points = pd.concat([cut_points, selected_columns], axis=0, ignore_index=True)\n\n# Now, data_list contains the data from the 13 specific CSV files with two-digit file numbers.\n\nprint(cut_points.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:34:28.475343Z","iopub.execute_input":"2024-11-19T08:34:28.476283Z","iopub.status.idle":"2024-11-19T08:34:28.546503Z","shell.execute_reply.started":"2024-11-19T08:34:28.476242Z","shell.execute_reply":"2024-11-19T08:34:28.545690Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ecg = df[0]\n    #print(ecg)\ncut_point = cut_points.iloc[0]\ncut_point1 = cut_point[0]\necg = ecg.to_frame()\necg.reset_index(drop=True, inplace=True)\nprint(ecg.T)\nrest = ecg.iloc[cut_point1:cut_point1+76800]\nrest.reset_index(drop=True, inplace=True)\nprint(rest.T)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:34:31.468525Z","iopub.execute_input":"2024-11-19T08:34:31.468850Z","iopub.status.idle":"2024-11-19T08:34:31.513488Z","shell.execute_reply.started":"2024-11-19T08:34:31.468823Z","shell.execute_reply":"2024-11-19T08:34:31.512529Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Here, LOOCV test-train splitting (only 1 subject is considered as validation data. the rests are as train data.) is done. Subjects =[0 to 12] sub = 0 means subject 1 of the Vollmer dataset will be considered as test data, while other subjects will be considered as train data.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nresult_test = pd.DataFrame()\nlabels_test = []\nresult_train = pd.DataFrame()\nlabels_train = []\nfor i in range(13):\n    ecg = df[i]\n    #print(ecg)\n    cut_point = cut_points.iloc[i]\n    cut_point1 = cut_point[0]\n    ecg = ecg.to_frame()\n\n    sub = 2 # subject no 0\n    # 0 to 12\n    if i == sub:\n        rest = ecg.iloc[cut_point1:cut_point1+76800]\n        rest.reset_index(drop=True, inplace=True)\n        result_test = pd.concat([result_test, rest], axis=1, ignore_index=True)\n        \n        stress = ecg.iloc[cut_point[2]:cut_point[2]+76800]\n        stress.reset_index(drop=True, inplace=True)\n        result_test = pd.concat([result_test, stress], axis=1, ignore_index=True)\n            \n        label1 = [0,1]\n        labels_test = labels_test+label1\n    elif i!=sub:\n        rest = ecg.iloc[cut_point1:cut_point1+76800]\n        rest.reset_index(drop=True, inplace=True)\n        result_train = pd.concat([result_train, rest], axis=1, ignore_index=True)\n        \n        stress = ecg.iloc[cut_point[2]:cut_point[2]+76800]\n        stress.reset_index(drop=True, inplace=True)\n        result_train = pd.concat([result_train, stress], axis=1, ignore_index=True)\n        label1 = [0,1]\n        labels_train = labels_train+label1\n \n\n\n    #walking = ecg.iloc[cut_point[1]:cut_point[1]+76800]\n    #walking.reset_index(drop=True, inplace=True)\n    #result = pd.concat([result, walking], axis=1, ignore_index=True)\n\n\n\n    #running = ecg.iloc[cut_point[3]:cut_point[3]+76800]\n    #running.reset_index(drop=True, inplace=True)\n    #result = pd.concat([result, running], axis=1, ignore_index=True)\n\n\nprint(result_train.shape)\nprint(labels_train)\nprint(result_test.shape)\nprint(labels_test)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:34:35.275127Z","iopub.execute_input":"2024-11-19T08:34:35.276086Z","iopub.status.idle":"2024-11-19T08:34:35.373904Z","shell.execute_reply.started":"2024-11-19T08:34:35.276054Z","shell.execute_reply":"2024-11-19T08:34:35.373081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"midpoint = int(76800/5)  # Adjust the value as needed\n\n# Split the result into two DataFrames\nfirst_half = result_train.iloc[:midpoint,: ]\nfirst_half.reset_index(drop=True, inplace=True)\n\nsecond_half = result_train.iloc[ midpoint:midpoint*2,:]\nsecond_half.reset_index(drop=True, inplace=True)\n\nthird_half = result_train.iloc[ midpoint*2:midpoint*3,:]\nthird_half.reset_index(drop=True, inplace=True)\n\nfourth_half = result_train.iloc[ midpoint*3:midpoint*4,:]\nfourth_half.reset_index(drop=True, inplace=True)\n\nfifth_half = result_train.iloc[ midpoint*4:midpoint*5,:]\nfifth_half.reset_index(drop=True, inplace=True)\n#print(fifth_half)\n# Concatenate the two DataFrames\nX_trai = pd.concat([first_half, second_half,third_half, fourth_half, fifth_half], axis=1, ignore_index=True)\ny_tra = labels_train+labels_train+labels_train+labels_train+labels_train\ny_train = pd.DataFrame(y_tra)\nX_train = X_trai.T\nprint(X_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:06:36.359137Z","iopub.execute_input":"2024-11-19T09:06:36.359442Z","iopub.status.idle":"2024-11-19T09:06:36.376998Z","shell.execute_reply.started":"2024-11-19T09:06:36.359419Z","shell.execute_reply":"2024-11-19T09:06:36.376203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting the train samples into 10 seconds segments.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming filtered_data is your DataFrame\n# Split the DataFrame into two halves along the columns\n#this is for 30 sec segment 15360\n\nfirst = X_train.iloc[:, :2560]\nfirst1 = X_train.iloc[:, 2560:5120]\nfirst_aug1 = X_train.iloc[:, 768:3328]\n#first_aug2 = X_train.iloc[:, 1792:4352]\nsecond = X_train.iloc[:, 5120:7680]\nprint(second.shape)\nsecond1 = X_train.iloc[:, 7680:10240]\nsecond_aug1 = X_train.iloc[:, 5888:8448]\n#second_aug2 = X_train.iloc[:, 6912:9472]\nthird = X_train.iloc[:, 10240:12800]\nthird1 = X_train.iloc[:, 12800:]\nthird_aug1 = X_train.iloc[:, 11008:13568]\n#third_aug2 = X_train.iloc[:, 12032:14592]\n\n# Stack the two halves along the rows\nresult_train = np.vstack((first, first1, second, second1, third, third1, first_aug1,\n                    second_aug1,third_aug1))\n\n# Verify the shape of the resulting array\nprint(result_train.shape)  # This should print (260, 7680)\n\nrepeated_y_train = np.tile(y_train, (12, 1))\n\n# Verify the shape of the resulting array\nprint(repeated_y_train.shape) ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:16:38.064051Z","iopub.execute_input":"2024-11-19T09:16:38.064420Z","iopub.status.idle":"2024-11-19T09:16:38.079317Z","shell.execute_reply.started":"2024-11-19T09:16:38.064392Z","shell.execute_reply":"2024-11-19T09:16:38.078318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting the test samples into 10 seconds segments.","metadata":{}},{"cell_type":"code","source":"midpoint = int(76800/5)  # Adjust the value as needed\n\n# Split the result into two DataFrames\nfirst_half = result_test.iloc[:midpoint,: ]\nfirst_half.reset_index(drop=True, inplace=True)\n\nsecond_half = result_test.iloc[ midpoint:midpoint*2,:]\nsecond_half.reset_index(drop=True, inplace=True)\n\nthird_half = result_test.iloc[ midpoint*2:midpoint*3,:]\nthird_half.reset_index(drop=True, inplace=True)\n\nfourth_half = result_test.iloc[ midpoint*3:midpoint*4,:]\nfourth_half.reset_index(drop=True, inplace=True)\n\nfifth_half = result_test.iloc[ midpoint*4:midpoint*5,:]\nfifth_half.reset_index(drop=True, inplace=True)\n#print(fifth_half)\n# Concatenate the two DataFrames\nX_tes = pd.concat([first_half, second_half,third_half, fourth_half, fifth_half], axis=1, ignore_index=True)\ny_tes = labels_test+labels_test+labels_test+labels_test+labels_test\ny_test = pd.DataFrame(y_tes)\nX_test = X_tes.T\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:17:14.742903Z","iopub.execute_input":"2024-11-19T09:17:14.743180Z","iopub.status.idle":"2024-11-19T09:17:14.752884Z","shell.execute_reply.started":"2024-11-19T09:17:14.743129Z","shell.execute_reply":"2024-11-19T09:17:14.752066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming filtered_data is your DataFrame\n# Split the DataFrame into two halves along the columns\n#this is for 30 sec segment 15360\n\nfirst = X_test.iloc[:, :2560]\nfirst1 = X_test.iloc[:, 2560:5120]\nsecond = X_test.iloc[:, 5120:7680]\nprint(second.shape)\nsecond1 = X_test.iloc[:, 7680:10240]\nthird = X_test.iloc[:, 10240:12800]\nthird1 = X_test.iloc[:, 12800:]\n\n# Stack the two halves along the rows\nresult_test = np.vstack((first, first1, second, second1, third, third1))\n\n# Verify the shape of the resulting array\nprint(result_test.shape)  # This should print (260, 7680)\n\nrepeated_y_test = np.tile(y_test, (6, 1))\n\n# Verify the shape of the resulting array\nprint(repeated_y_test.shape) ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:17:20.642926Z","iopub.execute_input":"2024-11-19T09:17:20.643524Z","iopub.status.idle":"2024-11-19T09:17:20.650986Z","shell.execute_reply.started":"2024-11-19T09:17:20.643494Z","shell.execute_reply":"2024-11-19T09:17:20.650088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repeated_y_train = np.array(repeated_y_train)\ny_test = np.array(repeated_y_test)\nfrom sklearn.preprocessing import StandardScaler,scale,MaxAbsScaler\nscaling=StandardScaler()\nX_train=scaling.fit_transform(result_train)\nX_test=scaling.transform(result_test)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:17:27.867465Z","iopub.execute_input":"2024-11-19T09:17:27.867787Z","iopub.status.idle":"2024-11-19T09:17:28.267756Z","shell.execute_reply.started":"2024-11-19T09:17:27.867759Z","shell.execute_reply":"2024-11-19T09:17:28.266664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Check shapes of X_train and repeated_y_train\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of repeated_y_train: {repeated_y_train.shape}\")\n\n# Ensure repeated_y_train is a 1-dimensional array\nif repeated_y_train.ndim > 1:\n    repeated_y_train = repeated_y_train.ravel()\n\n# Use boolean indexing to split the data\nX_majority = X_train[repeated_y_train == 0, :]\nX_minority = X_train[repeated_y_train == 1, :]\n\nprint(f\"Shape of X_majority: {X_majority.shape}\")\nprint(f\"Shape of X_minority: {X_minority.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:17:46.741665Z","iopub.execute_input":"2024-11-19T09:17:46.742497Z","iopub.status.idle":"2024-11-19T09:17:46.760227Z","shell.execute_reply.started":"2024-11-19T09:17:46.742462Z","shell.execute_reply":"2024-11-19T09:17:46.759418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking of minority class. As it was balanced dataset, no augmentation was done.","metadata":{}},{"cell_type":"code","source":"X_minority_augmented = np.zeros((2*X_minority.shape[0], 2304))\nprint(X_minority_augmented.shape)\nj=0\ni=0\n#print(int(X_minority.shape[0]/2))\nfor i in range(int(X_minority.shape[0]/2)):\n    i = i*2\n    #print(i)\n    x1=X_minority[i,:]\n    x2=X_minority[i+1,:]\n    x_combined = np.concatenate((X_minority[i,:], X_minority[i+1,:]))\n    #print(x_combined.shape)\n    x3 = x_combined[768:3072]\n    x4 = x_combined[1792:4096]\n    #print(x3.shape)\n    #print(x2.shape)\n        \n    X_minority_augmented[j] = x1  #np.roll(x1, 2000)\n    j=j+1    \n    X_minority_augmented[j] = x2  #np.roll(x1, 2000)\n    j=j+1\n    X_minority_augmented[j] = x3  #np.roll(x1, 2000)\n    j=j+1\n    #x1=X_minority[i,:]\n    X_minority_augmented[j] = x4 #np.roll(x1, 2000)x1\n    j=j+1\n    #i=i+1\n    #print(i)\n#print(i)\nprint(X_minority_augmented.shape)\n\nX_majority_augmented = X_majority\nX_minority_augmented = X_minority\nprint(X_majority_augmented.shape)\nprint(X_minority_augmented.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:18:18.894044Z","iopub.execute_input":"2024-11-19T09:18:18.894953Z","iopub.status.idle":"2024-11-19T09:18:18.940718Z","shell.execute_reply.started":"2024-11-19T09:18:18.894918Z","shell.execute_reply":"2024-11-19T09:18:18.939853Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_augmented = np.vstack((X_majority_augmented, X_minority_augmented))\ny_train_augmented = np.hstack((np.zeros(X_majority_augmented.shape[0]), np.ones(X_minority_augmented.shape[0])))\n# shuffle the data\n\nidx = np.random.permutation(X_train_augmented.shape[0])\n#print(idx)\nX_train_augmented = X_train_augmented[idx]\ny_train_augmented = pd.DataFrame(y_train_augmented[idx])\nprint(X_train_augmented.shape)\n#print(y_train_augmented[0])\nX_train=X_train_augmented\ny_train=y_train_augmented","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:18:27.533352Z","iopub.execute_input":"2024-11-19T09:18:27.533672Z","iopub.status.idle":"2024-11-19T09:18:27.560814Z","shell.execute_reply.started":"2024-11-19T09:18:27.533647Z","shell.execute_reply":"2024-11-19T09:18:27.559971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:18:30.619666Z","iopub.execute_input":"2024-11-19T09:18:30.620654Z","iopub.status.idle":"2024-11-19T09:18:30.625693Z","shell.execute_reply.started":"2024-11-19T09:18:30.620614Z","shell.execute_reply":"2024-11-19T09:18:30.624884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Buildup","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D, Attention, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\n# Define the input layer\ninput_layer = Input(shape=(2560, 1))\n\n# Shared CNN layers\ndef shared_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    conv1 = Conv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n\n    conv2 = Conv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n\n    conv3 = Conv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n\n    return batch_norm3\n\n# Shared BiLSTM layer with attention\ndef shared_bilstm_with_attention(shared_cnn_output, units, return_sequences, num_heads):\n    bilstm_output = Bidirectional(LSTM(units=units, return_sequences=return_sequences))(shared_cnn_output)\n    attention_heads = []\n    for _ in range(num_heads):\n        attention_head = Attention()([bilstm_output, bilstm_output])\n        attention_heads.append(attention_head)\n\n    # Concatenate the outputs of attention heads\n    multi_attention = Concatenate(axis=-1)(attention_heads)\n    global_pool = GlobalMaxPooling1D()(multi_attention)\n    return global_pool\n\n# Apply shared CNN and BiLSTM with attention to each input path with different parameters\nshared_cnn_output1 = shared_cnn(input_layer, filters1=64, kernel_size1=10, filters2=128, kernel_size2=5, filters3=256, kernel_size3=3)\nshared_bilstm_output1 = shared_bilstm_with_attention(shared_cnn_output1, units=64, return_sequences=True, num_heads=4)\n\nshared_cnn_output2 = shared_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\nshared_bilstm_output2 = shared_bilstm_with_attention(shared_cnn_output2, units=32, return_sequences=True, num_heads=2)\n\n# Concatenate the outputs of both paths\nconcatenated_output = Concatenate(axis=-1)([shared_bilstm_output1, shared_bilstm_output2])\n\n# Dense layers\ndense1 = Dense(units=256, activation='relu', kernel_regularizer=l2(0.01))(concatenated_output)\nbatch_norm4 = BatchNormalization()(dense1)\ndrop1 = Dropout(0.4)(batch_norm4)\n\ndense2 = Dense(units=128, activation='relu', kernel_regularizer=l2(0.01))(drop1)\nbatch_norm5 = BatchNormalization()(dense2)\ndrop2 = Dropout(0.4)(batch_norm5)\n\n# Output layer\noutput = Dense(units=1, activation='sigmoid')(drop2)\n\n# Model\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:18:38.767273Z","iopub.execute_input":"2024-11-19T09:18:38.767968Z","iopub.status.idle":"2024-11-19T09:18:51.644515Z","shell.execute_reply.started":"2024-11-19T09:18:38.767941Z","shell.execute_reply":"2024-11-19T09:18:51.643608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\n# Define early stopping and checkpoint callbacks\nes = EarlyStopping(monitor='val_loss', mode='min', patience=100)\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n# Train the model with early stopping and checkpoint callbacks\nhistory = model.fit(X_train, y_train, epochs=600, batch_size=32, validation_data=(X_test, y_test), callbacks=[es, mc])","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:24:14.154849Z","iopub.execute_input":"2024-11-19T09:24:14.155401Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calculating the confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom keras.models import load_model\n# Load the saved model\nmodel = load_model('best_model.h5')\n\n# Predict the class probabilities for the test set\ny_pred = model.predict(X_test)\n\n# Convert the probabilities into class labels using a threshold of 0.5\ny_pred_classes = (y_pred > 0.5).astype(int)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred_classes)\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T20:11:47.537831Z","iopub.execute_input":"2024-11-11T20:11:47.538203Z","iopub.status.idle":"2024-11-11T20:11:52.354697Z","shell.execute_reply.started":"2024-11-11T20:11:47.538171Z","shell.execute_reply":"2024-11-11T20:11:52.353496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss vs Epoch curve plotting","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# Print loss vs. epoch curve\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T18:58:12.933133Z","iopub.execute_input":"2024-11-11T18:58:12.933478Z","iopub.status.idle":"2024-11-11T18:58:13.238144Z","shell.execute_reply.started":"2024-11-11T18:58:12.933449Z","shell.execute_reply":"2024-11-11T18:58:13.237127Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AUC score calculating","metadata":{}},{"cell_type":"code","source":"# Calculate predictions for the test set\ny_pred = model.predict(X_test)\n\n# Calculate AUC score\nauc_score = roc_auc_score(y_test, y_pred)\n\n# Print AUC score\nprint('AUC Score:', auc_score)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T18:58:13.239453Z","iopub.execute_input":"2024-11-11T18:58:13.239773Z","iopub.status.idle":"2024-11-11T18:58:13.376905Z","shell.execute_reply.started":"2024-11-11T18:58:13.239745Z","shell.execute_reply":"2024-11-11T18:58:13.375860Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ROC curve plotting","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Calculate predictions for the test set\ny_pred = model.predict(X_test)\n\n# Calculate false positive rate, true positive rate, and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n# Calculate AUC score\nauc_score = roc_auc_score(y_test, y_pred)\n\n# Plot ROC curve\nplt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc_score))\nplt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T18:58:13.378265Z","iopub.execute_input":"2024-11-11T18:58:13.378701Z","iopub.status.idle":"2024-11-11T18:58:13.841738Z","shell.execute_reply.started":"2024-11-11T18:58:13.378656Z","shell.execute_reply":"2024-11-11T18:58:13.840724Z"},"trusted":true},"outputs":[],"execution_count":null}]}